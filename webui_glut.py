# coding=utf-8

import os
import gradio as gr
import numpy as np
import torch
import torchaudio
from funasr import AutoModel
import gc

class FunASRApp:
    def __init__(self):
        self.model = None


    def load_model(
            self, 
            model, 
            vad_model="fsmn-vad", 
            vad_kwargs={"max_single_segment_time": 30000}, 
            punc_model="ct-punc",  
            spk_model="cam++",
            device="cuda:0",
            disable_update=True,
    ):
        gc.collect()
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
        if model == "ç©ºè½½æ¨¡å‹":
            self.model = None
            print("æ¨¡å‹å·²å¸è½½")
            return "æ¨¡å‹å·²å¸è½½", gr.update(interactive=True)
        elif model == "æƒ…æ„Ÿæ¨¡å‹":
            punc_model = None
            spk_model = None
        
        print(f"å¼€å§‹åŠ è½½{model}")
        model_abbr = {"çƒ­è¯æ¨¡å‹": "iic/speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch", "æƒ…æ„Ÿæ¨¡å‹": "iic/SenseVoiceSmall"}        
        model = model_abbr[model]
        self.model = AutoModel(
            model=model,  
            vad_model=vad_model, 
            vad_kwargs=vad_kwargs,
            punc_model=punc_model, 
            spk_model=spk_model,
            device=device,
            disable_update=disable_update,
        )
        print(f"åŠ è½½{model}æˆåŠŸ")
        return "æ¨¡å‹åŠ è½½å®Œæˆ", gr.update(interactive=True)


    def model_inference(
            self, 
            model,
            input, 
            language, 
            use_itn=True,
            batch_size_s=60, 
            merge_vad=True,
            merge_length_s=15,
            sentence_timestamp=True,
            hotword='å¥½å“¥å“¥',
            fs=16000
    ):
        if self.model is None:
            return "è¯·å…ˆé€‰æ‹©å¹¶åŠ è½½æ¨¡å‹"
        if model == "æƒ…æ„Ÿæ¨¡å‹":
            sentence_timestamp = False
        
        gc.collect()
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
        language_abbr = {"auto": "auto", "zh": "zh", "en": "en", "yue": "yue", "ja": "ja", "ko": "ko", "nospeech": "nospeech"}
        language = "auto" if len(language) < 1 else language
        language = language_abbr[language]

        if isinstance(input, tuple):
            fs, input = input
            input = input.astype(np.float32) / np.iinfo(np.int16).max
            if len(input.shape) > 1:
                input = input.mean(-1)
            if fs != 16000:
                print(f"audio_fs: {fs}")
                resampler = torchaudio.transforms.Resample(fs, 16000)
                input_t = torch.from_numpy(input).to(torch.float32)
                input = resampler(input_t[None, :])[0, :].numpy()
        
        res = self.model.generate(
            input=input,
            cache={},
            language=language,
            use_itn=use_itn,
            batch_size_s=batch_size_s, 
            merge_vad=merge_vad,
            merge_length_s=merge_length_s,
            sentence_timestamp=sentence_timestamp,
            hotword=hotword,
        )
        print(res)
        text = res[0]["text"]
        return res, text


    def save_txt(self, text):
        save_path = os.path.join("outputs", "text_files", "output.txt")
        with open(save_path, "w") as f:
            f.write(text)
        return "Text saved to " + save_path


    def launch(self):
        with gr.Blocks(theme=gr.themes.Soft(), fill_height=True) as demo:
            gr.HTML(html_content)
            with gr.Row():
                with gr.Column():
                    audio_inputs = gr.Audio(label="ä¸Šä¼ éŸ³é¢‘æˆ–ä½¿ç”¨éº¦å…‹é£")
                    with gr.Accordion(label="é…ç½®"):
                        model_inputs = gr.Dropdown(label="æ¨¡å‹", choices=["çƒ­è¯æ¨¡å‹", "æƒ…æ„Ÿæ¨¡å‹", "ç©ºè½½æ¨¡å‹"], value="ç©ºè½½æ¨¡å‹")
                        status_text = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", value="æ¨¡å‹æœªåŠ è½½", interactive=False, visible=False)
                        language_inputs = gr.Dropdown(label="è¯­è¨€", choices=["auto", "zh", "en", "yue", "ja", "ko", "nospeech"], value="auto")
                    fn_button = gr.Button("å¼€å§‹", variant="primary")
                with gr.Column():
                    res_outputs = gr.Textbox(label="ç»“æœ", visible=False)
                    text_outputs = gr.Textbox(label="ç»“æœ")
                    #txt_outputs = gr.Button("å¯¼å‡ºtxt", variant="primary")
                    #srt_outputs = gr.Button("å¯¼å‡ºsrt", variant="primary")
            model_inputs.change(self.load_model, inputs=model_inputs, outputs=[status_text, model_inputs])
            fn_button.click(self.model_inference, inputs=[model_inputs, audio_inputs, language_inputs], outputs=[res_outputs, text_outputs])
            #txt_outputs.click(self.save_txt, inputs=[text_outputs])
            #srt_outputs.click(self.model_inference, inputs=[res_outputs,])

        demo.launch(inbrowser=True, share=False)

html_content = """
<div>
    <h2 style="font-size: 22px;text-align: center;">FunASRåº”ç”¨ç¨‹åº FunASR-webui</h2>
</div>
<div style="text-align: center; font-size: 15px; font-weight: bold; color: red;">
    âš ï¸ è¯¥æ¼”ç¤ºä»…ä¾›å­¦æœ¯ç ”ç©¶å’Œä½“éªŒä½¿ç”¨ã€‚
</div>
<div style="text-align: center;">
    åˆ¶ä½œ by åå­—é±¼|
    <a href="https://space.bilibili.com/893892">ğŸŒ Bilibili</a> 
</div>
"""

if __name__ == "__main__":
    app = FunASRApp()
    app.launch()